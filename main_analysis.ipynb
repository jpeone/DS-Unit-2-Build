{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bosch production line performance\n",
    "This and all dependant notebooks are attempting to inform strategic decision making based off of the data collected on Bosch's manufacturing lines.\n",
    "\n",
    "### Goals\n",
    "To put myself in the mindset of a manufacturer, glean as much manufacturing insight as possible from this data set.  Additionally to use hypothesis testing as a method to systematically find and explain my best analysis\n",
    "\n",
    "### Challenges\n",
    "This data set is almost entirely randomized. Catgoricals have no meaning to me, continuous data as meaningful as measurements and time stamps.  \n",
    "This set is HUGE. It is very unlikely that I will be able to use this entire set with my current resources (home pc).\n",
    "\n",
    "### References\n",
    "[Dataset and description](https://www.kaggle.com/c/bosch-production-line-performance/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Case\n",
    "This set contains a series of measurements, cateogries, and timestamps relating to a specific part traveling down Bosch's manufacturing lines. Ultimately this part will be classified as passing or failing a quality check.  \n",
    "### Target\n",
    "I'm looking to predict if a part will pass or fail QC. According to Bosch's documentation it is the feature ```Response```\n",
    "\n",
    "### Problem Type\n",
    "Since it is either pass or fail, I'll be working on ```binary classification```\n",
    "\n",
    "### Metric\n",
    "To know what metric, I need to understand how this data is distributed and understand the business consequences of our predictions. This will require some analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1183747,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    0\n",
       "2    0\n",
       "3    0\n",
       "4    0\n",
       "Name: Response, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Okay, lets get started by taking a look at my distributions.  I know from bosch\n",
    "#documentation that our target is in the numeric dataset.\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "folder = 'bosch-production-line-performance/'\n",
    "\n",
    "response = pd.read_csv(folder + 'train_numeric.csv', usecols = ['Response'],\n",
    "                      squeeze = True)\n",
    "\n",
    "print(response.shape)\n",
    "response.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1176868\n",
       "1       6879\n",
       "Name: Response, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#okay lets look at that distribution\n",
    "response.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99.0% of our parts passed QC\n"
     ]
    }
   ],
   "source": [
    "percent = round(response.value_counts()[0] / response.shape[0] * 100)\n",
    "\n",
    "print(f'{percent}% of our parts passed QC')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribution\n",
    "Okay this set is extremely imbalanced, so accuracy is not going to be a good default metric.  So lets knock out a confusion matrix, and see if we can put some stakes to these predictions.\n",
    "\n",
    "#### Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|                 | Predicted Failed                                                                                                                                          | Predicted Passed                                                                                                                                     |\n",
    "|-----------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| Actually Failed | Sweet                                                                                                                                                     | Part goes to production, product made, possibly fails in field, loss of consumer faith, high complexity cost and monetary cost of reverse logistics. Total material costs of other parts in final defective product |\n",
    "| Actually Passed | Part gets binned for rework.  Best case, is identified as good during rework. Worst case classified as wastage. I'd expect lower overall cost and impact. | Sweet                                                                                                                                                |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metric: Precision\n",
    "It seems to me the cost of letting false positives slip through the line is much higher. So we need to prioritize precision. I will also be tracking MCC as a secondary metric, since it will illuminate my models overall performance. Additionally it is the scoring method of the original competition this came from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(y_true, y_pred):\n",
    "    #We are manually setting our positive label, because our data set\n",
    "    #We are also setting zero division to 0, because a dataset this imbalance\n",
    "    #is very likely generate warnings otherwise.\n",
    "    print('Our Precision is: ', precision_score(y_true, y_pred, pos_label = 0, \n",
    "                                               zero_division = 0))\n",
    "    print('Our MCC is: ', matthews_corrcoef(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis tests\n",
    "I will update this grid in place as I complete each test. You may need to reference other notebooks to see more indepth supporting work. These hypothesis were guided by the following principles:  \n",
    "* __Dimension Reduction__ - I only have so much RAM available, I need as many observations as possible due to highly imbalanced set  \n",
    "* __Numeric features are the most important__ - The set is fuller, and the most predictive in initial exploration  \n",
    "* __Must be explainable__ - as few \"black boxes\" as possible. When used, at least use them on a class of data to help maintain storytelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| |Hypothesis|Action|Results|Insight|Reference|\n",
    "|-|----------|------|-------|-------|---------|\n",
    "|0|A model that beats a guess is a good starting baseline|Create a ```uniform``` baseline measured by ```precision```| 0.0056 |A guess is a bad way to run QC|This notebook|\n",
    "|1|A massive data set made of the combination of numeric, categorical, and timestamp sets would allow the most information for making predictions|Merge datasets and run a few simple models|0.0|Memory footprint was too large, for too few observations. Reverting to split data sets|wrangle_1_megaset.ipynb|\n",
    "|2.0|A ```logistic regression``` model run on the date.csv set will beat baseline and can be stacked as a metafeature|Clean train_date.csv, fit basic model, eval|0.0|Was unpredictive, try to improve|wrangle_2_metafeature_pca_date.ipynb|\n",
    "|2.1|logistic regression model could be improved by focusing on high performing features, and removing low performing features|performed ```permutation importance```|0.0|No single feature was improved or diminished by permutation importance, model wasn't improved, try a different linear model| \"|\n",
    "|2.2|```Boosting``` with linear learners would beat baseline|fit basic XGBoost model, eval|0.0|unpredictive. This data set may not be predictive enough on its own. Changing approach|\"|\n",
    "|2.3|Set may be relationally important to train_numeric.csv set|use PCA to reduce train_date.csv|na|Haven't tested yet| this notebook|\n",
    "|3.0|A ```Random Forest Classifier``` run on the categorical data set would beat baseline|cleaned train_categorical.csv, fit basic RFC model| .33 | First major success, attempt to improve|wrangle_3_explore_categoricals.ipynb|\n",
    "|3.1|Using ```boosting``` with a random forest will improve model|fit a basic XGBRFClassifier model| 0.0 |boosting reduced predictions significantly, reverted to previous model| \" |\n",
    "|3.2| Random permutation could help me identify under or over performing features| performed ```permutation importance``` on the forest model| 0.33| Very low impact on features, reverted to base model| \" |\n",
    "|3.3| The RFC model's feature importance could help me identify the highest performing features| fit a new RFC model with only features of > 0 importance | .20| Performance went down. Dropped well over 1k features. Likely thousands of minisculy low impact features is still better a couple hundred mediocer features and one high impact feature|\"|\n",
    "|3.4| ```Stacking``` the results and probabilities from RFC model with my numeric data set will improve performance|create metafeature with predictions and probabilities| na| haven't tested yet| This notebook|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
